# -*- coding: utf-8 -*-
"""NLP_CODE.ipynb

Automatically generated by Colaboratory. NLP Project Group 9

Original file is located at
    https://colab.research.google.com/drive/1DXwJNpJDX20epFF5Uwg3sfRWP3xnV2_4
"""

!pip install visualise-spacy-tree

import pandas as pd
import spacy
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Image, display


from spacy.matcher import Matcher 
from spacy import displacy 
from random import randint
from pathlib import Path

import glob
import visualise_spacy_tree
import re

folders = glob.glob('/content/drive/MyDrive/Converted sessions/Session*')


df = pd.DataFrame(columns={'Country','Speech','Session','Year'})


i = 0 
for file in folders:
    
    speech = glob.glob(file+'/IND*.txt')
    if len(speech)<1: continue
    with open(speech[0],encoding='utf8') as f:
        df.loc[i,'Speech'] = f.read()
        df.loc[i,'Year'] = speech[0].split('_')[-1].split('.')[0]
        df.loc[i,'Session'] = speech[0].split('_')[-2]
        df.loc[i,'Country'] = speech[0].split('_')[0].split("/")[-1]
        i += 1 
        
df.head()

df.loc[0,'Speech']


def clean(text):
    
    text = re.sub('[0-9]+.\t','',str(text))
    text = re.sub('\n ','',str(text))
    text = re.sub('\n',' ',str(text))
    text = re.sub("'s",'',str(text))
    text = re.sub("-",' ',str(text))
    text = re.sub("â€” ",'',str(text))
    text = re.sub('\"','',str(text))
    text = re.sub("Mr\.",'Mr',str(text))
    text = re.sub("Mrs\.",'Mrs',str(text))
    text = re.sub("[\(\[].*?[\)\]]", "", str(text))
    
    return text


df['Speech_clean'] = df['Speech'].apply(clean)


def sentences(text):
    text = re.split('[.?]', text)
    clean_sent = []
    for sent in text:
        clean_sent.append(sent)
    return clean_sent


df['sent'] = df['Speech_clean'].apply(sentences)

df.head()


df2 = pd.DataFrame(columns=['Sent','Year','Len'])


row_list = []


for i in range(len(df)):
    for sent in df.loc[i,'sent']:
        
        wordcount = len(sent.split()) 
        year = df.loc[i,'Year']
        dict1 = {'Year':year,'Sent':sent,'Len':wordcount}
        row_list.append(dict1)
    

df2 = pd.DataFrame(row_list)
df2.head()
df2.shape




nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])


def find_names(text):
    
    names = []
    
    doc = nlp(text)
    
    pattern = [{'LOWER':'prime'}, {'LOWER':'minister'},{'POS':'ADP','OP':'?'},{'POS':'PROPN'}]
                
    matcher = Matcher(nlp.vocab) 
    matcher.add("names", None, pattern) 

    matches = matcher(doc)

    
    for i in range(0,len(matches)):
        
        
        token = doc[matches[i][1]:matches[i][2]]
        names.append(str(token))
    
    
    for name in names:
        if (name.split()[2] == 'of') and (name.split()[3] != "India"):
                names.remove(name)
            
    return names

df2['PM_Names'] = df2['Sent'].apply(find_names)


for i in range(len(df2)):
    if df2.loc[i,'Year'] in ['1984']:
        if len(df2.loc[i,'PM_Names'])!=0:
            print('->',df2.loc[i,'Sent'],'\n')

count=0
for i in range(len(df2)):
    if len(df2.loc[i,'PM_Names'])!=0:
        count+=1
print(count)





def prog_sent(text):
    
    patterns = [r'\b(?i)'+'plan'+r'\b',
               r'\b(?i)'+'programme'+r'\b',
               r'\b(?i)'+'scheme'+r'\b',
               r'\b(?i)'+'campaign'+r'\b',
               r'\b(?i)'+'initiative'+r'\b',
               r'\b(?i)'+'conference'+r'\b',
               r'\b(?i)'+'agreement'+r'\b',
               r'\b(?i)'+'alliance'+r'\b']

    output = []
    flag = 0
  
    for pat in patterns:
        if re.search(pat, text) != None:
            flag = 1
            break
    return flag 

df2['Check_Schemes'] = df2['Sent'].apply(prog_sent)



count = 0
for i in range(len(df2)):
    if df2.loc[i,'Check_Schemes'] == 1:
        count+=1
print(count)

def all_schemes(text,check):
    
    schemes = []
    
    doc = nlp(text)
    
    
    prog_list = ['programme','scheme',
                 'initiative','campaign',
                 'agreement','conference',
                 'alliance','plan']
    
    pattern = [{'POS':'DET'},
               {'POS':'PROPN','DEP':'compound'},
               {'POS':'PROPN','DEP':'compound'},
               {'POS':'PROPN','OP':'?'},
               {'POS':'PROPN','OP':'?'},
               {'POS':'PROPN','OP':'?'},
               {'LOWER':{'IN':prog_list},'OP':'+'}
              ]
    
    if check == 0:
        return schemes

    matcher = Matcher(nlp.vocab) 
    matcher.add("matching", None, pattern) 
    matches = matcher(doc)

    for i in range(0,len(matches)):
        start, end = matches[i][1], matches[i][2]
        
        if doc[start].pos_=='DET':
            start = start+1

        span = str(doc[start:end])
        
        if (len(schemes)!=0) and (schemes[-1] in span):
            schemes[-1] = span
        else:
            schemes.append(span)
        
    return schemes

df2['Schemes1'] = df2.apply(lambda x:all_schemes(x.Sent,x.Check_Schemes),axis=1)

count = 0
for i in range(len(df2)):
    if len(df2.loc[i,'Schemes1'])!=0:
        count+=1
print(count)

year = '2018'
for i in range(len(df2)):
    if df2.loc[i,'Year']==year:
        if len(df2.loc[i,'Schemes1'])!=0:
            print('->',df2.loc[i,'Year'],',',df2.loc[i,'Schemes1'],':')
            print(df2.loc[i,'Sent'])

doc = nlp('Prime Minister Modi, together with the Prime Minister of France, launched the International Solar Alliance')
png = visualise_spacy_tree.create_png(doc)
display(Image(png))

doc = nlp(' Last year, I spoke about the Ujjwala programme , through which, I am happy to report, 50 million free liquid-gas connections have been provided so far')
png = visualise_spacy_tree.create_png(doc)
display(Image(png))

doc = nlp('Prime Minister Modi, together with the Prime Minister of France, launched the International Solar Alliance')
png = visualise_spacy_tree.create_png(doc)
display(Image(png))

def sent_subtree(text):
    
    patterns = [r'\b(?i)'+'plan'+r'\b',
           r'\b(?i)'+'programme'+r'\b',
           r'\b(?i)'+'scheme'+r'\b',
           r'\b(?i)'+'campaign'+r'\b',
           r'\b(?i)'+'initiative'+r'\b',
           r'\b(?i)'+'conference'+r'\b',
           r'\b(?i)'+'agreement'+r'\b',
           r'\b(?i)'+'alliance'+r'\b']
    
    schemes = []
    doc = nlp(text)
    flag = 0
    
    for pat in patterns:
        
        if re.search(pat, text) != None:
            flag = 1
            break
    
    if flag == 0:
        return schemes

    
    for token in doc:

        for pat in patterns:
                
            if re.search(pat, token.text) != None:

                word = ''
                for node in token.subtree:
                    
                    if (node.pos_ == 'PROPN'):
                        word += node.text+' '

                if len(word)!=0:
                    schemes.append(word)

    return schemes      


df2['Schemes2'] = df2['Sent'].apply(sent_subtree)

count = 0
for i in range(len(df2)):
    if len(df2.loc[i,'Schemes2'])!=0:
        count+=1
print(count)

year = '2018'
for i in range(len(df2)):
    if df2.loc[i,'Year']==year:
        if len(df2.loc[i,'Schemes2'])!=0:
            print('->',df2.loc[i,'Year'],',',df2.loc[i,'Schemes2'],':')
            print(df2.loc[i,'Sent'])

row_list = []

for i in range(len(df2)):
    sent = df2.loc[i,'Sent']
    
    if (',' not in sent) and (len(sent.split()) <= 15):
        
        year = df2.loc[i,'Year']
        length = len(sent.split())
        
        dict1 = {'Year':year,'Sent':sent,'Len':length}
        row_list.append(dict1)
        

df3 = pd.DataFrame(columns=['Year','Sent',"Len"])
df3 = pd.DataFrame(row_list)

df3.head()


def rand_sent(df):
    
    index = randint(0, len(df))
    print('Index = ',index)
    doc = nlp(df.loc[index,'Sent'][1:])
    displacy.render(doc, style='dep',jupyter=True)
    
    return index

rand_sent(df3)

def output_per(df,out_col):
    
    result = 0
    
    for out in df[out_col]:
        if len(out)!=0:
            result+=1
    
    per = result/len(df)
    per *= 100
    
    return per



text = df3.loc[9,'Sent'][1:]

doc = nlp(text)
img = displacy.render(doc, style='dep',jupyter=True)
img


def rule1(text):
    
    doc = nlp(text)
    
    sent = []
    
    for token in doc:
        
        
        if (token.pos_=='VERB'):
            
            phrase =''
            
            
            for sub_tok in token.lefts:
                
                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):
                    
                    
                    phrase += sub_tok.text

                    
                    phrase += ' '+token.lemma_ 

                    
                    for sub_tok in token.rights:
                        
                        
                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):
                                    
                            phrase += ' '+sub_tok.text
                            sent.append(phrase)
            
    return sent


row_list = []

for i in range(len(df3)):
    
    sent = df3.loc[i,'Sent']
    year = df3.loc[i,'Year']
    output = rule1(sent)
    dict1 = {'Year':year,'Sent':sent,'Output':output}
    row_list.append(dict1)
    
df_rule1 = pd.DataFrame(row_list)


output_per(df_rule1,'Output')


row_list = []


for i in range(len(df2)):
    
    sent = df2.loc[i,'Sent']
    year = df2.loc[i,'Year']
    output = rule1(sent)
    dict1 = {'Year':year,'Sent':sent,'Output':output}
    row_list.append(dict1)
    
df_rule1_all = pd.DataFrame(row_list)


output_per(df_rule1_all,'Output')


df_show = pd.DataFrame(columns=df_rule1_all.columns)

for row in range(len(df_rule1_all)):
    
    if len(df_rule1_all.loc[row,'Output'])!=0:
        df_show = df_show.append(df_rule1_all.loc[row,:])


df_show.reset_index(inplace=True)
df_show.drop('index',axis=1,inplace=True)        

df_rule1_all.shape, df_show.shape



verb_dict = dict()
dis_dict = dict()
dis_list = []


for i in range(len(df_show)):
    
    
    sentence = df_show.loc[i,'Sent']
    
    year = df_show.loc[i,'Year']
    
    output = df_show.loc[i,'Output']
    
    
    for sent in output:
        
        
        n1 = sent.split()[:1]
        v = sent.split()[1]
        n2 = sent.split()[2:]
        
        
        dis_dict = {'Sent':sentence,'Year':year,'Noun1':n1,'Verb':v,'Noun2':n2}
        dis_list.append(dis_dict)
        
        
        verb = sent.split()[1]
        if verb in verb_dict:
            verb_dict[verb]+=1
        else:
            verb_dict[verb]=1

df_sep = pd.DataFrame(dis_list)

df_sep.head()

sort = sorted(verb_dict.items(), key = lambda d:(d[1],d[0]), reverse=True)

sort[:10]


df_sep[df_sep['Verb']=='support']



df_sep[df_sep['Verb']=='face']

text = 'Our people are expecting a better life.'
print(text)
doc = nlp(text)
img = displacy.render(doc, style='dep',jupyter=True)
img

def rule2(text):
    
    doc = nlp(text)

    pat = []
    
    
    for token in doc:
        phrase = ''
        
        if (token.pos_ == 'NOUN')\
            and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass']):
            
            
            for subtoken in token.children:
                
                if (subtoken.pos_ == 'ADJ') or (subtoken.dep_ == 'compound'):
                    phrase += subtoken.text + ' '
                    
            if len(phrase)!=0:
                phrase += token.text
             
        if  len(phrase)!=0:
            pat.append(phrase)
        
    
    return pat


row_list = []

for i in range(len(df3)):
    
    sent = df3.loc[i,'Sent']
    year = df3.loc[i,'Year']
    
    output = rule2(sent)
    
    dict1 = {'Year':year,'Sent':sent,'Output':output}
    row_list.append(dict1)

df_rule2 = pd.DataFrame(row_list)

df_rule2.head()


output_per(df_rule2,'Output')


row_list = []


for i in range(len(df2)):
    
    sent = df2.loc[i,'Sent']
    year = df2.loc[i,'Year']
    output = rule2(sent)
    dict1 = {'Year':year,'Sent':sent,'Output':output}
    row_list.append(dict1)
    
df_rule2_all = pd.DataFrame(row_list)


output_per(df_rule2_all,'Output')

df_rule2_all.head(10)


df_show2 = pd.DataFrame(columns=df_rule2_all.columns)

for row in range(len(df_rule2_all)):
    
    if len(df_rule2_all.loc[row,'Output'])!=0:
        df_show2 = df_show2.append(df_rule2_all.loc[row,:])


df_show2.reset_index(inplace=True)
df_show2.drop('index',axis=1,inplace=True)  

df_show2.head(10)
